# moderation/ai_tools.py
import os
import requests
import json
from dotenv import load_dotenv

load_dotenv()
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def ai_analyze_report(title: str, description: str, resource_url: str = None) -> dict:
    """
    Analyze a report for NSFW, plagiarism, and AI-generated content.
    Returns:
        - is_plagiarism: bool
        - ai_confidence: float
        - ai_flags: list of tags
        - risk_label: str ("Low", "Medium", "High")
        - is_nsfw: bool
    """

    text_to_analyze = f"{title}\n{description}".lower()

    # --- Mock logic for demo / testing ---
    # NSFW detection
    nsfw_keywords = ["adult", "sex", "explicit"]
    is_nsfw = any(word in text_to_analyze for word in nsfw_keywords)

    # AI detection
    ai_keywords = ["as an advanced ai language model", "generated by ai", "ai-optimized"]
    ai_confidence = min(sum(text_to_analyze.count(k) for k in ai_keywords) / max(len(text_to_analyze.split()), 1), 1.0)

    # Plagiarism detection
    plag_keywords = ["according to the united nations report", "copied from", "source:"]
    is_plagiarism = any(word in text_to_analyze for word in plag_keywords)

    # Build tags
    ai_flags = []
    if ai_confidence > 0.4:
        ai_flags.append("AI-generated")
    if is_nsfw:
        ai_flags.append("NSFW")
    if is_plagiarism:
        ai_flags.append("Plagiarism")
    if ai_confidence > 0.7 or is_nsfw or is_plagiarism:
        risk_label = "High"
    elif ai_confidence > 0.5:
        risk_label = "Medium"
    else:
        risk_label = "Low"

    return {
        "is_plagiarism": is_plagiarism,
        "ai_confidence": ai_confidence,
        "ai_flags": ai_flags,
        "risk_label": risk_label,
        "is_nsfw": is_nsfw
    }
